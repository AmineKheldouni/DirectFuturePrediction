game.get_last_reward() 
game.is_terminated -> reward -96/4 selon dead/en vie




1° pourquoi l'agent ne pick jamais de poison
ou alors c'est son inférence qui est merdique
if (prev_misc[0] - misc[0] > 8): # Pick up Poison
            poison += 1

2 °
For comparison sake, I also ran the same number of episodes using Double DQN (DDQN). A scalar reward of +1 is given to the agent when it picks up Health Packs, and a negative penalty of -100 for dying. Average survival time (moving average over 50 episodes) is used as the metric for performance
dans DFP il a l'air de juste faire : r_t = game.get_last_reward()
mais dans DDQN il fait ca
r_t = game.get_last_reward()  ; r_t = agent.shape_reward_d1(r_t, misc, prev_misc, t) où il "enrichit la reward" cf. article de l'autre blog "By default, a death penalty of -1 is provided by the environment. In order to facilitate learning, I enriched the variety of rewards (reward shaping) to include a +1 reward for every kill, and a -0.1 reward for losing ammo and health. "
Ainsi dans l'article qui nous intéresse, faut-il enrichir que pour DDQN ? (pas pour DFP ?).

[3° pourquoi la health est pas nulle quand il crève ?]

[4° By default, a death penalty of -1 is provided by the environment. Pourtant quand je regarde la reward apreès is_terminated j'ai -96. at'il deja modifié l'environnement de game.last_reward() ?]




je suis surement idiot mais pourquoi la reward peut etre positive quand is_terminated ? et le temps de survie est-il borné ?

j'ai mal expliqué. en gros dans dqn il prend pas la reward de l'environnement (-1 si mort) mais la détaille en fonction des mesures. je me demandais pourquoi il le faisait pas pour dfp

 La health est nulle mais on la voit pas parce qu'on ne print pas le dernier état de l'agent lors du dernier épisode du jeu.° j'ai pas pigé. j'obtiens dans dfp print ("Episode Finish ", misc)-> misc=4

peut etre entre ddqn et dfp, mais dfp avec 3 mesures est plus mauvais que dfp avec 1 mesure ce qui me semble toutefois normal vu que ya pas de poison




----------------
EXP
PARTIE PREDICTIVE TEST


UNDERSTAND REWARD.
les valeurs de life,poison,...

NETWORK FOR MULTIPLE CHANNELS !
ANTICIPATION.
----------------

TODO :

pour la phase de test faudrait juste rajouter un self.test=False/True, self.n_tests et commenter la loss pour True je pense

en gros c'est le temps pour que epsilon descende à 0
donc en fait apres self.explore, on prend l'action optimale mais on train quand meme
de 0 à self.observe on prend eps=1 et on train pas
de self.observe à self.observe+self.explore
on train mais on decroit eps de 1 à 0
après ca on train avec eps=0


\section{CSV file}

Exp : stats + weights

Params experience :
- goal
- episode où change goals

Train. Criteres perf.:
- t
- measurements de l'exp (health, health pack, poison)
- health
- reward (sum jusq'à t, t2=0 à chaque fin d'épisode) 

\section{Experiments}

DFP/DDQN 2 env D1 et D3

Effet ajout mesures
D1 : [Health]
D1 : [Health, HealthPacks, Poison].
D1 : [Health, HealthPacks, distance nearest HealthPack, Poison]

Effet difficulté du but par rapport à exp1
D3 : [Health, Frags, Amo]
D3 : [Health, Frags, Amo, distance ennemy, distance healthpack].

--- 
CHANGEMENT BUT DURANT LE TRAIN sur D3 (Amine)

D3 : [Health, Frags, Amo]

- Fixed goal : Si on ne change pas de but
 CF ARTICLE
- Change but 
  CF ARTICLE
A la fin, on veut savoir sur le but TestGoal (plusieurs tests si possible) lequel a gagné.

(changer de but => meilleurs résultats ?)

----
ANTICIPATION

D1 [1,2,4]
VS
D1 [1,2,4,8,16,32]
VS
D1 [1,2,4,8,16,32,64] (cf token Google)

---------


Amine : taille vecteur actions
action_size = game.get_available_buttons_size()
